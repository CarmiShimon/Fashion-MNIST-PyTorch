{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will be doing classification of images using both DNN and Convolutional Neural Networks \n",
        "\n",
        "We will be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) Fashion-MNIST is a set of 28x28 grayscale images of clothes. dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels)\n",
        "\n",
        "![Fashion MNIST dataset](https://github.com/abdulelahsm/ignite/blob/update-tutorials/examples/notebooks/assets/fashion-mnist.png?raw=1)"
      ],
      "metadata": {
        "id": "87rHuVqAw9fL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-56h06d0uN4m",
        "outputId": "3ded40ea-b931-4276-c465-c42363a39fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/PhD_Carmi_Shimon/deep_learning_course\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "% cd './drive/My Drive/PhD_Carmi_Shimon/deep_learning_course'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import"
      ],
      "metadata": {
        "id": "I1W6xQTnvM-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "# Load the TensorBoard notebook extension\n",
        "import tensorflow as tf\n",
        "from tensorflow import summary\n",
        "%load_ext tensorboard\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "6WbW3Oy6vN8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Util Functions"
      ],
      "metadata": {
        "id": "yXkRSUQH5lh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_samples(data_loader):\n",
        "  ''' plots random 20 images in a data_loader''' \n",
        "  dataiter = iter(data_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  fig = plt.figure(figsize=(15, 5))\n",
        "  for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(4, 20/4, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(labels[idx].item())\n",
        "    fig.tight_layout()\n",
        "\n",
        "def show_results(y_pred, y_true, class_names):\n",
        "  cm = confusion_matrix(y_pred, y_true)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  disp.plot(ax=ax)\n",
        "  plt.show()\n",
        "  test_accuracy = accuracy_score(y_true, y_pred)\n",
        "  print('Test Accuracy = ', test_accuracy)\n",
        "  accuracies = cm.diagonal()/cm.sum(axis=1)\n",
        "  tuple_accuracies = [(name, round(acc, 2)) for name, acc in zip(class_names, accuracies)]\n",
        "  print('per class test accuracy: \\n', tuple_accuracies)\n",
        "\n",
        "def plot_tsne(embeddings, y):\n",
        "  Xt = TSNE(n_components=2, n_jobs=8, n_iter=300).fit_transform(embeddings)\n",
        "  plt.scatter(Xt[:, 0], Xt[:, 1], c=y.astype(np.int32),\n",
        "                                  s=0.5)"
      ],
      "metadata": {
        "id": "GYOi6JYxLe8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Trainer"
      ],
      "metadata": {
        "id": "MZRB-3JGLhxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "  def __init__(self, model, transform, batch_size=64, val_size=0.2, lr=0.001, is_show_plots=False, n_classes=10):\n",
        "    os.makedirs('./saved_models', exist_ok=True)\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.n_classes = n_classes\n",
        "    self.changed_labels = {0: [0, 2, 3, 4, 6], 1: [1, 5, 7, 9], 2: [8]}\n",
        "    self.model = model.to(self.device)\n",
        "    self.criterion = nn.NLLLoss()\n",
        "    self.optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    self.batch_size = batch_size\n",
        "    self.val_size = val_size\n",
        "    self.is_show_plots = is_show_plots\n",
        "    # PIL image in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    self.test_transform = transforms.Compose([transforms.ToTensor()]) \n",
        "    self.train_transform = transform\n",
        "    self.__dataLoader()\n",
        "\n",
        "  def __dataLoader(self):\n",
        "    # Download and load the train and test data\n",
        "    trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=self.train_transform)\n",
        "    testset = datasets.FashionMNIST('./data', download=True, train=False, transform=self.test_transform)\n",
        "    # --------- 1.2 Dataset Manipulation - take care of 3 classes mode ------------ #\n",
        "    if self.n_classes == 3:\n",
        "      for i, vals in enumerate(list(self.changed_labels.values())):\n",
        "        for val in vals:\n",
        "          trainset.train_labels[trainset.train_labels == val] = list(self.changed_labels.keys())[i]\n",
        "          testset.test_labels[testset.test_labels == val] = list(self.changed_labels.keys())[i]\n",
        "    # ----------------------------------------------------------------------------- # \n",
        "    # validation set preparation\n",
        "    indices = list(range(len(trainset)))\n",
        "    np.random.shuffle(indices)\n",
        "    # get val_size % from train set as val set\n",
        "    split = int(np.floor((1-self.val_size) * len(trainset)))\n",
        "    train_sample = SubsetRandomSampler(indices[:split])\n",
        "    valid_sample = SubsetRandomSampler(indices[split:])\n",
        "\n",
        "    # Data Loader\n",
        "    self.train_loader = DataLoader(trainset, sampler=train_sample, batch_size=self.batch_size)\n",
        "    self.val_loader = DataLoader(trainset, sampler=valid_sample, batch_size=self.batch_size)\n",
        "    self.test_loader = DataLoader(testset, batch_size=self.batch_size, shuffle=True)\n",
        "    if self.is_show_plots:\n",
        "      plot_samples(self.train_loader)\n",
        "\n",
        "# ----------------Train-------------------- #\n",
        "  def train(self, epochs):\n",
        "    min_val_loss = 2\n",
        "    log_interval = 10\n",
        "    self.model.train() # Set the model in train mode\n",
        "    train_losses, val_losses = [], []\n",
        "    train_acc, val_acc = 0, 0\n",
        "    # train loop\n",
        "    for e in range(epochs):\n",
        "      running_loss, val_loss = 0, 0\n",
        "      for images, labels in self.train_loader:\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        self.optimizer.zero_grad() # Clear the gradients of all optimized variables\n",
        "        out, _ = self.model(images) # Forward pass through the network\n",
        "        _, pred = torch.max(out, 1)\n",
        "        loss = self.criterion(out, labels) # calculate the loss\n",
        "        loss.backward() # Backward pass through the network to calculate the gradients for model parameters\n",
        "        self.optimizer.step() # Take a step with the optimizer to update the model parameters\n",
        "        running_loss += loss.item()*images.size(0) # aggregate batch loss\n",
        "        train_acc += torch.sum(labels == pred).item()\n",
        "\n",
        "      # val loop\n",
        "      self.model.eval() # Set the model in train mode\n",
        "      for batch_idx, (images, labels) in enumerate(self.val_loader):\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        out, _ = self.model(images)\n",
        "        _, pred = torch.max(out, 1)\n",
        "        loss = self.criterion(out, labels)\n",
        "        val_loss += loss.item()*images.size(0)\n",
        "        val_acc += torch.sum(labels == pred).item()\n",
        "\n",
        "      # performance measure\n",
        "      train_acc /= len(self.train_loader.sampler)\n",
        "      val_acc /= len(self.val_loader.sampler)\n",
        "      running_loss = running_loss/len(self.train_loader.sampler)\n",
        "      val_loss = val_loss/len(self.val_loader.sampler)\n",
        "      train_losses.append(running_loss)\n",
        "      val_losses.append(val_loss)\n",
        "\n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f} \\t Train Acc: {:.6f} \\t Val Loss: {:.6f} \\t Val Acc: {:.6f}'\\\n",
        "            .format(e+1, running_loss, train_acc, val_loss, val_acc))\n",
        "       \n",
        "       # ------------ write to tensorboard log-file ----------- #\n",
        "      with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('Loss', running_loss, step=e)\n",
        "            tf.summary.scalar('Accuracy', train_acc, step=e)\n",
        "      with val_summary_writer.as_default():\n",
        "            tf.summary.scalar('Loss', val_loss, step=e)\n",
        "            tf.summary.scalar('Accuracy', val_acc, step=e)\n",
        "        \n",
        "      \n",
        "      # in case a model gets better, save weights\n",
        "      if val_loss <= min_val_loss:\n",
        "        print('validation loss decreased({:.6f} -->{:.6f}). Saving Model ...'\\\n",
        "              .format(min_val_loss, val_loss))\n",
        "        model_name = f'DNN_epoch_{e+1}.pt'\n",
        "        torch.save(self.model.state_dict(), model_name)\n",
        "        min_val_loss = val_loss\n",
        "\n",
        "\n",
        "  def test(self):\n",
        "    test_loss = 0\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    self.model.eval()\n",
        "    for images, labels in self.test_loader:\n",
        "      images, labels = images.to(self.device), labels.to(self.device)\n",
        "      out, _ = self.model(images) # Forward pass\n",
        "      loss = self.criterion(out, labels) # calculate loss\n",
        "      test_loss += loss.item()*images.size(0) # update test loss\n",
        "      _, pred = torch.max(out, 1)\n",
        "      # collect preds and true labels\n",
        "      preds += [p.item() for p in pred]\n",
        "      true_labels += [l.item() for l in labels]\n",
        "    return preds, true_labels\n",
        "\n",
        "  def get_embeddings(self):\n",
        "    self.model.eval()\n",
        "    train_embds, test_embds = np.array([], dtype=np.int64).reshape(0,128), np.array([], dtype=np.int64).reshape(0,128)\n",
        "    y_train, y_test = [], []\n",
        "    for images, labels in self.train_loader:\n",
        "      images, labels = images.to(self.device), labels.to(self.device)\n",
        "      out, embds = self.model(images) # Forward pass\n",
        "      train_embds = np.vstack([train_embds, embds.cpu().data.numpy()])\n",
        "      y_train += [l.item() for l in labels]\n",
        "    for images, labels in self.test_loader:\n",
        "      images, labels = images.to(self.device), labels.to(self.device)\n",
        "      out, embds = self.model(images) # Forward pass\n",
        "      test_embds = np.vstack([test_embds, embds.cpu().data.numpy()])\n",
        "      y_test += [l.item() for l in labels]\n",
        "    train_embds, y_train = np.asarray(train_embds), np.asarray(y_train)\n",
        "    test_embds, y_test = np.asarray(test_embds), np.asarray(y_test)\n",
        "\n",
        "    return train_embds, y_train, test_embds, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "3xa4QOxp5lpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN models"
      ],
      "metadata": {
        "id": "ccA6WANE6fQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 DNN class as the base model"
      ],
      "metadata": {
        "id": "KnWtGog4635l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes=10):\n",
        "    super(DNN, self).__init__()\n",
        "    self.flatten = nn.Flatten(1)\n",
        "    self.fc1 = nn.Linear(28*28, 128)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    embds = x # latent vector of size 128 for better representation\n",
        "    x =  self.fc2(x)\n",
        "    return F.log_softmax(x,dim=1), embds"
      ],
      "metadata": {
        "id": "vvbaS0ej6jyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 CNN class - different model configurations"
      ],
      "metadata": {
        "id": "P0zdyxYA95jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1(nn.Module):\n",
        "    def __init__(self, n_classes=10):\n",
        "      super(CNN1, self).__init__()\n",
        "      self.n_classes = n_classes\n",
        "      self.convlayer1 = nn.Sequential(\n",
        "          nn.Conv2d(1, 32, 3, padding=1),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "     \n",
        "      self.fc1 = nn.Linear(32*28*28,600)\n",
        "      self.fc2 = nn.Linear(600, 128)\n",
        "      self.fc3 = nn.Linear(128, self.n_classes)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.convlayer1(x)\n",
        "      x = x.view(-1,32*28*28)\n",
        "      x = self.fc1(x)\n",
        "      x = self.fc2(x)\n",
        "      embds = x\n",
        "      x = self.fc3(x)\n",
        "      \n",
        "      return F.log_softmax(x,dim=1), embds\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self, n_classes=10):\n",
        "      super(CNN2, self).__init__()\n",
        "      self.n_classes = n_classes\n",
        "      self.convlayer1 = nn.Sequential(\n",
        "          nn.Conv2d(1, 32, 3, padding=1),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "      )\n",
        "     \n",
        "      self.fc1 = nn.Linear(32*14*14,600)\n",
        "      self.drop = nn.Dropout2d(0.3)\n",
        "      self.fc2 = nn.Linear(600, 128)\n",
        "      self.fc3 = nn.Linear(128, self.n_classes)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.convlayer1(x)\n",
        "      x = x.view(-1,32*14*14)\n",
        "      x = self.fc1(x)\n",
        "      x = self.drop(x)\n",
        "      x = self.fc2(x)\n",
        "      embds = x\n",
        "      x = self.fc3(x)\n",
        "      \n",
        "      return F.log_softmax(x,dim=1), embds\n",
        "      \n",
        "class CNN3(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_classes=10):\n",
        "        super(CNN3, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.convlayer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.convlayer2 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(64*6*6,600)\n",
        "        self.drop = nn.Dropout2d(0.3)\n",
        "        self.fc2 = nn.Linear(600, 128)\n",
        "        self.fc3 = nn.Linear(128, self.n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.convlayer1(x)\n",
        "        x = self.convlayer2(x)\n",
        "        x = x.view(-1,64*6*6)\n",
        "        x = self.fc1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        embds = x\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return F.log_softmax(x,dim=1), embds\n",
        "\n",
        "class CNN4(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_classes=10):\n",
        "        super(CNN4, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.convlayer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.convlayer2 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.convlayer3 = nn.Sequential(\n",
        "            nn.Conv2d(64,128,3),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(128*2*2,600)\n",
        "        self.drop = nn.Dropout2d(0.3)\n",
        "        self.fc2 = nn.Linear(600, 128)\n",
        "        self.fc3 = nn.Linear(128, self.n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.convlayer1(x)\n",
        "        x = self.convlayer2(x)\n",
        "        x = self.convlayer3(x)\n",
        "        x = x.view(-1, 128*2*2)\n",
        "        x = self.fc1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        embds = x\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return F.log_softmax(x,dim=1), embds"
      ],
      "metadata": {
        "id": "_IUWBMf56fZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorBoard files**"
      ],
      "metadata": {
        "id": "qQvxhXreNwmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_time = str(datetime.datetime.now().timestamp())\n",
        "train_log_dir = 'logs/tensorboard/train/' + current_time\n",
        "val_log_dir = 'logs/tensorboard/val/' + current_time\n",
        "train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "val_summary_writer = summary.create_file_writer(val_log_dir)\n"
      ],
      "metadata": {
        "id": "9AW8yOTlNwuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function"
      ],
      "metadata": {
        "id": "ZUqy1kkX_rBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  # CONSTANTS\n",
        "  is_show_plots = False\n",
        "  is_plot_tsne = False\n",
        "  BATCH_SIZE=64\n",
        "  VAL_SIZE=0.2\n",
        "  EPOCH=10\n",
        "  np.random.seed(123)\n",
        "\n",
        "  # --------------------------------------------------------- #               \n",
        "  # --------------- 1.1 - Vanilla Training ------------------ #  \n",
        "  # creating model, defining optimizer and loss\n",
        "  NUM_CLASSES = 10\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "  model = DNN(n_classes=NUM_CLASSES)\n",
        "  transform = transforms.Compose([transforms.ToTensor()]) \n",
        "  # instantiate Trainer class\n",
        "  trainer = Trainer(model, transform, batch_size=BATCH_SIZE, val_size=VAL_SIZE, is_show_plots=True, n_classes=NUM_CLASSES)\n",
        "  %tensorboard --logdir logs/tensorboard\n",
        "  # train model\n",
        "  trainer.train(epochs=EPOCH)\n",
        "  # run test on unseen data\n",
        "  y_pred, y_true = trainer.test()\n",
        "  show_results(y_pred, y_true, class_names)\n",
        "  train_embds, y_train, test_embds, y_test = trainer.get_embeddings()\n",
        "  if is_plot_tsne:\n",
        "    plot_tsne(train_embds, np.expand_dims(np.asarray(y_train), axis=1))\n",
        "\n",
        "  # --------------------------------------------------------- #               \n",
        "  # ----------------- 1.2 Dataset Manipulation -------------- #               \n",
        "  NUM_CLASSES=3\n",
        "  class_names = ['Top', 'Bottom', 'Accessories']\n",
        "  model = DNN(n_classes=NUM_CLASSES)\n",
        "  # instantiate Trainer class\n",
        "  trainer = Trainer(model, transform, batch_size=BATCH_SIZE, val_size=VAL_SIZE, is_show_plots=True, n_classes=NUM_CLASSES)\n",
        "  %tensorboard --logdir logs/tensorboard\n",
        "  # train model\n",
        "  trainer.train(epochs=EPOCH)\n",
        "  # run test on unseen data\n",
        "  y_pred, y_true = trainer.test()\n",
        "  show_results(y_pred, y_true, class_names)\n",
        "  train_embds, y_train, test_embds, y_test = trainer.get_embeddings()\n",
        "  if is_plot_tsne:\n",
        "    plot_tsne(train_embds, np.expand_dims(np.asarray(y_train), axis=1))\n",
        "    plot_tsne(test_embds, np.expand_dims(np.asarray(y_test), axis=1))\n",
        "  \n",
        "  # # --------------------------------------------------------- #               \n",
        "  # # ----------------- 1.3 Data Augmentation ----------------- #    \n",
        "  # is_show_plots = False\n",
        "  # AUGMENTATIONS=['noise', 'cropping', 'rotation', 'horizontal_flip', 'affine', 'scaling', 'deformation', 'brightness', 'contrast']\n",
        "  # NUM_CLASSES=3\n",
        "  # class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "  #              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "  # # instantiate Trainer class\n",
        "  # trainer = Trainer(model, transform, batch_size=BATCH_SIZE, val_size=VAL_SIZE, is_show_plots=True, n_classes=NUM_CLASSES)\n",
        "  # # train model\n",
        "  # trainer.train(epochs=EPOCH)\n",
        "  # # run test on unseen data\n",
        "  # y_pred, y_true = trainer.test()\n",
        "  # show_results(y_pred, y_true, class_names)\n",
        "\n",
        "  # --------------------------------------------------------- #               \n",
        "  # ----------------- 1.4 Choosing a Model ------------------ #  \n",
        "  model = CNN3(n_classes=NUM_CLASSES)\n",
        "  transform = transforms.Compose([transforms.ToTensor()]) \n",
        "  # instantiate Trainer class\n",
        "  trainer = Trainer(model, transform, batch_size=BATCH_SIZE, val_size=VAL_SIZE, is_show_plots=True, n_classes=NUM_CLASSES)\n",
        "  # train model\n",
        "  %tensorboard --logdir logs/tensorboard\n",
        "  trainer.train(epochs=EPOCH)\n",
        "  # run test on unseen data\n",
        "  y_pred, y_true = trainer.test()\n",
        "  show_results(y_pred, y_true, class_names)\n",
        "  train_embds, y_train, test_embds, y_test = trainer.get_embeddings()\n",
        "  if is_plot_tsne:\n",
        "    plot_tsne(train_embds, np.expand_dims(np.asarray(y_train), axis=1))\n",
        "    plot_tsne(test_embds, np.expand_dims(np.asarray(y_test), axis=1))"
      ],
      "metadata": {
        "id": "9O-gjDNx8wz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i9EIpQ2Ybgrz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}